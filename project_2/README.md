# <center> Проект № 2: Анализ резюме из HeadHunter

## Оглавление
[1. Описание проекта](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Описание-проекта)\
[2. Какой кейс решаем?](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Какой-кейс-решаем)\
[3. Краткая информация о данных](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Краткая-информация-о-данных)\
[4. Этапы работы над проектом](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Этапы-работы-над-проектом)\
[5. Результат](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Результаты)\
[6. Выводы](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Выводы)

## Описание проекта:

*Вам необходимо подбирать вакансии для IT-специалистов. Ваш проект — создание модели машинного обучения, которая будет рекомендовать вакансии клиентам агентства, претендующим на позицию Data Scientist. Сначала вам необходимо понять, что из себя представляют данные и насколько они соответствуют целям проекта. В литературе эта часть работы над ML-проектом называется Data Understanding, или анализ данных.*


:arrow_up:[к оглавлению](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Оглавление)

## Какой кейс решаем?:

- Создание модели машинного обучения, которая будет рекомендовать вакансии клиентам агентства, претендующим на позицию Data Scientist. Используя навыки написания запросов SQL и Postgers.

:arrow_up:[к оглавлению](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#-оглавление)

***Условия задания:***
- Каждая часть состоит из блока практических заданий, которые необходимо выполнить в jupyter-ноутбуке.
- Ноутбук необходимо оформить на основе предоставленного [шаблона](https://lms.skillfactory.ru/assets/courseware/v1/a39c1eedaae738f78d85c950f78223fa/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/Project_2_%D0%9D%D0%BE%D1%83%D1%82%D0%B1%D1%83%D0%BA_%D1%88%D0%B0%D0%B1%D0%BB%D0%BE%D0%BD.ipynb) и требований.
- Отправить свой код ментору для code-ревью.

***Требования к оформлению:***

- Решение оформляется только в Jupyter Notebook.
- Решение оформляется в соответствии с ноутбуком-шаблоном.
- Каждое задание выполняется в отдельной ячейке, выделенной под задание (в шаблоне они помечены как ваш код здесь). Не следует создавать много ячеек для решения задачи — это провоцирует неудобства при проверке.
- Текст SQL-запросов и код на Python должны быть читаемыми. Не забывайте про отступы в SQL-коде.
- Выводы по каждому этапу оформляются в формате Markdown в отдельной ячейке (в шаблоне они помечены как ваши выводы здесь).
- Выводы можно дополнительно проиллюстрировать с помощью графиков. Они оформляются в соответствии с теми правилами, которые мы приводили в модуле по визуализации данных.
- Не забудьте удалить ячейку с данными соединения перед фиксацией работы в GitHub.

:arrow_up:[к оглавлению](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Оглавление)

***Что практикуем:***

1. Работа с данными
2. Оформление отчетов
3. Работа и составление SQL запросов
4. Обработка полученных данных

### Краткая информация о данных:

- Данные представлены в Metabase  в виде пяти таблиц:
1. VACANCIES - Хранит в себе основные данные о вакансиях
2. AREAS- Таблица-справочник, которая хранит код города и его название
3. EMPLOYERS - Таблица-справочник со списком работодателей
4. INDUSTRIES - Таблица-справочник вариантов сфер деятельности работодателей
5. EMPLOYERS_INDUSTRIES Дополнительная таблица, которая существует для организации связи между работодателями и сферами их деятельности.
Эта таблица нужна нам, поскольку у одного работодателя может быть несколько сфер деятельности (или работодатели могут вовсе не указать их). Для удобства анализа необходимо хранить запись по каждой сфере каждого работодателя в отдельной строке таблицы

:arrow_up:[к оглавлению](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Оглавление)

### Этапы работы над проектом:

1. Знакомство с данными;
2. Предварительный анализ данных;
3. Детальный анализ вакансий;
4. Анализ работодателей;
5. Предметный анализ.

:arrow_up:[к оглавлению](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Оглавление)

### Результаты:

- [Ноутбук с резултаьами работы](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/Project_2.ipynb)


:arrow_up:[к оглавлению](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Оглавление)

### Выводы:

По результатам работы проведены все необходимые этапы, для получения модели для обработки ваканси Data Science.

Основные выводы по всему проекту: 
- *Исходя из проделанного анализа, мы можем сделать вывод о том, что на основе данных у нас имеется 49197 вакансий, в которых представленно 23501 работадатель, представленнных 1362 регионах и 294 сферах деятельности. По количеству вакансий размешенных в регионе, являются самыми крупными "Москва", "Санкт-Питербург", также из анализа мы видим что не все работадатели заполняют поле зарплаты (salary_from, salary_to), в нашем случае одно из полей заполнено у 24073 вакансий. Можно оценить что средняя зарплата начинается от 71065 и достигает 110537 рублей. Если говорить о том, какой график представляют работодатели, то самым частым в наших данных является "Полный день- Полная занятость" с результатом 35367 вакансий, затем идет "Удаленная работа - Полная занятость" с результатом 7802, из чего можно сделать вывод, что многие работадатели перешли на удаленный график работы, после короновирусных ограничений, и тенденция идет к увелечению удаленных вакансий. Тенденци о соискателях с опытом работы сохраняется, большинство работодателей предпочитают нанимать опытных сотрудником, чтобы не тратить время на их обучение.*
- *Самыми частыми работодателями представляющими свои вакансии является "Яндекс", "Ростелеком", "Тинькофф". В нашем варианте преимущественно вакансии из городов России. Также многие работодатели не указывают сферу своей деятельности (8419), что также для соискателей может быть не очень удобно, по причине того что они могут не понять чем конкретно занимается данная компания. Но некоторые из компаний указывают несколько сфер деятельности, так как данные компании развиваются в разных сферах, чтобы охватить большое количество сфер и работать комплексно.*
- *Говоря об анализе вакансий в сфере данных, можно сказать, что таких вакансий не так уж и много всего 1771 вакансия, из них всего 51 вакансия подходит начинающим аналитикам данных. Можно понять что одним из ключевых навыков является SQL или postgers, а также python  так как эти навыки необходимы в 201 и 351 вакансии соответственно. Сотрудник работающий с данными должен обладать большим количеством ключевых навыков, для проведения качественной работы, из анализа мы видим, что в среднем это 6-7 ключевых навыков необходимых для данных соискателей. Говоря о зарплатах можно сделать вывод, что в среднем работодатели готовы платить от 74643 рублей до 243115 рублей работником в зависимости от их опыта работы.*

:arrow_up:[к оглавлению](https://github.com/g1dcs/sf_hw_g1dcs/blob/main/project_2/README.md#Оглавление)